## Whole-Stage Code-Generation

Spark 2.x had an aggressive goal to get orders of magnitude faster performance. For such an aggressive goal, traditional techniques like using a profiler to identify hotspots and shaving those hotspots is not gonna help much. To understand this, as a motivating example, here’s one query:
![image](https://user-images.githubusercontent.com/22542670/27002100-face1ed2-4df5-11e7-8b23-90a0114ab120.png)
Its a very straight forward query. Basically, scan the entire sales table and outputs the items where item_id =512. The right hand side shows spark’s query plan for the same. Each of the stages shown in the query plan is an operator which performs a specific operation on the data like Filter, Count, Scan etc

### How does Spark 1.x evaluate this query plan?
**Ans: Volcano Iterator Model**

Spark SQL uses the traditional database technique which is called Volcano Iterator Model. 
This is a standard technique adapted in majority of the database systems for over 30years. As the name suggests `[IteratorModel]`, all the operators like filter, project, scan etc implement a common iterator interface and they all generate output in a common standard output format. Query plan shown on the right side is basically nothing but a list of operators chained together which are processed like this:
- Read the output generated by the parent operator
- Does some processing
- Produce the return value in a standard output row format.
- Hands over the return value to the next child operator
- Who is the child/parent of what is known only at runtime.
- Every handshake between 2 operators causes one virtual function call + reading parent output from memory + write the final output in memory
- In the example query plan shown on the right, Scan is the parent of the chain. Scan reads input data one-by-one, writes  the output in main memory and hands it over to the next child which is Filter function and so on..

### Downsides of Volcano Iterator Model:
- **Too many virtual function calls**
- - We don’t know where the child is coming from.
- - Its all dynamic dispatching between parent and child operators at runtime. 
- - Its agnostic to the operator below it.
- **Extensive memory access**
- - There’s a standard row format that exists between all the operators and this means writes to main memory. Potentially, you read a row in and send a new row to your parent.This suffers from problems in writing intermediate rows to main memory. 
- **Unable to leverage lot of modern techniques** like pipelining, prefetching, branch prediction, SIMD, loop unrolling etc..
- **Conclusion:** With VolcanoIterator Model, its difficult to get order’s of magnitude performance speed ups using the traditional profiling techniques. 

**Instead, let’s look bottom up..**

### What does look bottom-up mean?
A college freshman would implement the same query using a for-loop and if-condition like the one shown below:
![image](https://user-images.githubusercontent.com/22542670/27002119-a70c178a-4df6-11e7-830e-c6ebc7fdebae.png)

### Volcano model vs College freshman Code:
There’s ~10x speed difference between these 2 models
![image](https://user-images.githubusercontent.com/22542670/27002141-e088460a-4df6-11e7-9034-e354ea6d91ea.png)

### Why is the difference so huge?
College freshman hand-written code is very simple. It does exactly the work it needs to do. No virtual function calls. Data is in cpu registers and we are able to maximise the benefits of the compiler and hardware. 
**Key thing is:** Hand written code is taking advantage of all the information that is known. Its designed specifically to run that query and nothing else VS volcano model is a more generic one
![image](https://user-images.githubusercontent.com/22542670/27002155-0ce6a9bc-4df7-11e7-847f-ea26b942743b.png)

### Key IDEA is to come up with an execution engine which:
Has the functionality of a general-purpose execution engine like volcano model and
Perform just like a hand built system that does exactly what user wants to do.

### Okay! How do we get that? 
**Answer:** Whole-Stage Code Generation 
- This is a new technique now popular in DB literature.
- Basically, Fuse the operators in the query plan together so the generated code looks like hand optimised code as shown in the below picture:
![image](https://user-images.githubusercontent.com/22542670/27002225-b4b421c2-4df9-11e7-9cd5-222139e73e45.png)

### What does this mean?
- Identify chain of operators a.k.a stages
- Instead of having each operator as an individual function, combine and compile all of those stages into single function.
- At runtime generate the byte code that needs to be run

### Let’s take another example.. 
**Join with some filters and aggregation.**
- **Left hand side:** shows how the query plans look like in volcano iterator model. There are 9 virtual function calls with 8 intermediate results. 
- **Right hand side:** shows how whole-stage code generation happens for this case. Here, we reduced Number of function calls to 2 and Number of intermediate results to 1. Each of those 2 box’s shown on the right-hand side is going to be converted into a single java function.
There are different rules as to how we split up those pipelines depending on the usecase. 
![image](https://user-images.githubusercontent.com/22542670/27002256-a687774c-4dfa-11e7-84e3-90f187d2e62f.png)

### Observation:
Whole-stage Code Generation works particularly well when the operations we want to do are simple. But there are cases where it is infeasible to generate code to fuse the entire query into a single function like the one’s listed below:
- **Complicated I/O:**
- - Complicated parsing like CSV or parquet. 
- - We cant have the pipeline extend over the physical machines (Network I/O).
- **External Integrations:**
- - With third party components like python, tensor-flow etc,  we cant integrate their code into our code. 
- - Reading cached-data

### Is there anything that we can do to above mentioned stuff which can't be fused together in whole-stage code-generation?
Indeed Yes!!

### What did WSCG give us?
![image](https://user-images.githubusercontent.com/22542670/27023716-633e04e2-4f71-11e7-9930-539d25742e96.png)

### What extension can we add to this further?
**Goal2:** Speed up query execution..

### How can we speed up?
Ans: Vectorization

### What is Vectorization?
As main memory grew, query performance is more and more determined by raw CPU costs of query processing. That’s where vector operations came into picture to allow in-core parallelism for operations on arrays (vectors) of data via specialised instructions, vector registers and more FPU’s per core .

### Goal of Vectorization
Parallelise computations over vector arrays

### How to parallelise computations i.e., perform vector operations?
Two major approaches:
1. Pipelining
2. SIMD (Single Instruction Multiple Data)

### Quick peek on Pipelining and SIMD:
1. **Pipelining:** Pipelining execution involves making sure that different pipeline stages can simultaneously work on different instructions keeping dependencies among the instructions in mind to avoid stalls and result in throughput increase. Just like car assembly pipeline, hardware typically works in pipeline with various stages as shown below:
![image](https://user-images.githubusercontent.com/22542670/27023827-d5a43c40-4f71-11e7-9f96-fecbee92a37d.png)

So, pipeline basically does the following:
- Executes multiple different tasks simultaneously. 
- It uses large vectors 
- Spans many cycles per instruction.

One more example pipeline for a simple math operation like (x^2 + 8)/2:
![image](https://user-images.githubusercontent.com/22542670/27023935-5555db38-4f72-11e7-9511-0c4446138c8d.png)

2. **SIMD:**
	1. Executes several instances of a single task simultaneously
	2. It uses small vectors 
	3. Spans only few cycles per instruction
Let's look at how SIMD works on the same example (x^2 + 8)/2:
![image](https://user-images.githubusercontent.com/22542670/27024104-ebb3db8e-4f72-11e7-98ca-1d66b9b2c86e.png)
## What is critical to achieve best efficiency with Vector operations?
**Data Availability**

### How data availability becomes critical for execution speed?
To illustrate this better let’s look at an example and compare the same pipeline with and without CPU stall. 
**Pipeline without any CPU stall:** 
Following picture depicts an ideal pipeline of 4 instructions where everything is beautifully falling in-place and CPU is not idled:
![image](https://user-images.githubusercontent.com/22542670/27024152-15cced2a-4f73-11e7-963f-f3308af328d0.png)

**Pipeline with CPU stall:**
![image](https://user-images.githubusercontent.com/22542670/27024272-852bc452-4f73-11e7-8163-6c3e7a5c74be.png)

Above example clearly illustrates how data availability is very critical for efficient usage of pipeline. 

### Action Plan: 
- Better cache-utilisation, proper data alignment and memory latency contribute to DataAvailability issue we discussed above. - So, in order to have homogenous data architecture, spark moved from row-based storage format to support columnar in-memory data.

### What is Spark's Vectorization?
Spark's Vectorisation is nothing but batching multiple rows together in a columnar format, and each operator uses simple loops to iterate over data within a batch. Each next() call would thus return a batch of tuples, amortizing the cost of virtual function dispatches.  
**Performance bechmarking:** For benchmarking, Parquet which is the most popular columnar-format for hadoop stack was considered. Parquet scan performance in spark 1.6 ran at the rate of 11million/sec. Parquet vectored is basically directly scanning the data and materialising it in the vectorized way. Parquet vectorized ran at about 90 million rows/sec roughly 9x faster. This is promising and clearly shows that this is right thing to do.
![image](https://user-images.githubusercontent.com/22542670/27002326-b9833618-4dfc-11e7-9730-81306d0d0a4e.png)

**Downside of Vectorization:** Like we discussed in VolcanoIteratorModel, all the Intermediate results will be written to main memory. Because of this extensive memory access, where ever possible, Spark does ** WholeStageCodeGeneration first. **

### Summary: 
**We’ve explored following in this article:**
- How VolcanoIteratorModel in spark 1.x interprets and executes a given query
- Downsides of VolcanoIteratorModel like number of virtual function calls, excessive memory reads and writes happening for intermediate results etc
- Compared it with hand-written code and noticed easy 10x speedup!! Ola!!
- Hence came WholeStageCodeGeneration!
- But, WholeStageCodeGeneration cannot be done for complex operations. To handle these cases faster, Spark came up with Vectorization to better leverage the techniques of Modern CPU’s and hardware.
- Vectorization speeds up processing by batching multiple rows per instruction together and running them as SIMD instruction.
- **Conclusion:** Whole stage code generation has done decent job in combining the functionality of general-purpose execution engine. Vectorization is a good alternative for  the cases that are not handled by Whole-stage code-generation.
